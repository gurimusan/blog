<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gurimusan blog</title>
    <link>https://gurimusan.github.io/blog/</link>
    <description>Recent content on gurimusan blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-JP</language>
    <lastBuildDate>Thu, 21 Jun 2018 01:58:20 +0900</lastBuildDate>
    
	<atom:link href="https://gurimusan.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Murmur ハッシュ関数</title>
      <link>https://gurimusan.github.io/blog/2018/06/murmur-%E3%83%8F%E3%83%83%E3%82%B7%E3%83%A5%E9%96%A2%E6%95%B0/</link>
      <pubDate>Thu, 21 Jun 2018 01:58:20 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2018/06/murmur-%E3%83%8F%E3%83%83%E3%82%B7%E3%83%A5%E9%96%A2%E6%95%B0/</guid>
      <description>MurmurHash 2008年にAustin Applebyという人が考案した非暗号ハッシュ。
fnvハッシュより高速なハッシュ関数。
fnvハッシュは1byteずつ処理を行っているのに対し、Murmurハッシュは4byteずつ処理を行い、一度に処理するバイト数が増えるので高速になるというわけである。
RubyのHashTableの実装に使われているらしい。
MurmurHashはMurmurHash3というのが一番新しく、32ビットまたは128ビットのハッシュを生成する。
MurmurHash2は古いバージョンで、32ビットまたは64ビットのハッシュを生成すし、64ビットのハッシュ値生成するソースには2つの変種がある。64ビットプロセッサ用に最適化された用のMurmurHash64Aと32ビットプロセッサ用に最適化されたMurmurHash64Bである。
MurmurHash1は現在は廃止されたバージョン。
Murmurハッシュの最初のバージョンの実装 下記のコードはMurmurハッシュの最初のバージョンの実装。
最初の実装なのでシンプル。
MurmurHash
MurmurHash.cpp
//----------------------------------------------------------------------------- // MurmurHash, by Austin Appleby // Note - This code makes a few assumptions about how your machine behaves - // 1. We can read a 4-byte value from any address without crashing // 2. sizeof(int) == 4 // And it has a few limitations - // 1. It will not work incrementally. // 2.</description>
    </item>
    
    <item>
      <title>FNV-1a ハッシュ関数</title>
      <link>https://gurimusan.github.io/blog/2018/06/fnv-1a-%E3%83%8F%E3%83%83%E3%82%B7%E3%83%A5%E9%96%A2%E6%95%B0/</link>
      <pubDate>Tue, 19 Jun 2018 01:58:20 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2018/06/fnv-1a-%E3%83%8F%E3%83%83%E3%82%B7%E3%83%A5%E9%96%A2%E6%95%B0/</guid>
      <description>FNV-1a ハッシュ関数の実装。
キー値のそれぞれのバイトに対して、排他的論理和と乗算を1回ずつ行い、ハッシュ値を求めるアルゴリズム。
hash = offset_basis for each octet_of_data to be hashed hash = hash xor octet_of_data hash = hash * FNV_prime return hash  ハッシュ値のサイズによって、offset_basisとFNV_primeは値を変える。
offset_basis  32bit 2166136261 64bit 14695981039346656037 128bit 144066263297769815596495629667062367629 256bit 100029257958052580907070968620625704837092796014241193945225284501741471925557  FNV_prime  32bit $2^{24} + 2^8 + 0x93 = 16777619$ 64bit $2^{40} + 2^8 + 0xb3 = 1099511628211$ 128bit $2^{88} + 2^8 + 0x3b = 309485009821345068724781371$ 256bit $2^{168} + 2^8 + 0x63 = 374144419156711147060143317175368453031918731002211$  実装(golang) const offset32 = 2166136261 const prime32 = 16777619 func FNV32a(data []byte) uint32 { var hash uint32 = offset32 for _, c := range data { hash ^= uint32(c) hash *= prime32 } return hash }  参考 http://www.</description>
    </item>
    
    <item>
      <title>PRML 2.3.1 条件付き確率分布</title>
      <link>https://gurimusan.github.io/blog/2018/04/prml-2.3.1-%E6%9D%A1%E4%BB%B6%E4%BB%98%E3%81%8D%E7%A2%BA%E7%8E%87%E5%88%86%E5%B8%83/</link>
      <pubDate>Wed, 25 Apr 2018 13:49:43 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2018/04/prml-2.3.1-%E6%9D%A1%E4%BB%B6%E4%BB%98%E3%81%8D%E7%A2%BA%E7%8E%87%E5%88%86%E5%B8%83/</guid>
      <description>PRML 2.3.1 条件付き確率分布 PRML難しい。
分からないところ理解するとために情報を整理する。
今回は「2.3.1 条件付き確率分布」の内容を整理する。
この節でいいたいこと 多変量ガウス分布の特性の１つとして、節の最初に下記にのように述べられている。
 2つの変数集合の同時分布がガウス分布に従うなら、一方の変数集合が与えられた時の、もう一方の集合の条件付き分布もガウス分布に従う  これを具体的に書くと
 2つの変数集合$\boldsymbol{x_a}$, $\boldsymbol{x_b}$の同時分布$p(\boldsymbol{x_a},\boldsymbol{x_b})$がガウス分布に従うなら、一方の変数集合$\boldsymbol{x_b}$が与えられた時の、もう一方の集合の条件付き分布$p(\boldsymbol{x_a}|\boldsymbol{x_b})$もガウス分布に従う  ということ。
条件付き分布がガウス分布であるということ 多変量ガウスは $$ N(\boldsymbol{x}|\boldsymbol{\mu},\Sigma) = \frac{1}{(2\pi)^{\frac{D}{2}}} \frac{1}{| \Sigma |^{\frac{1}{2}}} \exp\left(-\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu})^{\mathrm{T}} \Sigma^{-1} (\boldsymbol{x} - \boldsymbol{\mu}) \right) \tag{2.43} $$ となる。
多変量ガウスがD次元として、ベクトル$\boldsymbol{x}$を２つの互いに素な部分集合$\boldsymbol{x_{a}}$と$\boldsymbol{x_{b}}$に分割する。
ここで$\boldsymbol{x_{a}}$は$\boldsymbol{x}$の最初の$M$個の要素で、$\boldsymbol{x_{b}}$は残りの$D-M$個の要素で構成する。
$$ \boldsymbol{x} = \begin{pmatrix} \boldsymbol{x_{a}} \\\ \boldsymbol{x_{b}} \end{pmatrix} \tag{2.65} $$
平均ベクトル$\boldsymbol{\mu}$の分割も定義する。 $$ \boldsymbol{\mu} = \begin{pmatrix} \boldsymbol{\mu_{a}} \\\ \boldsymbol{\mu_{b}} \end{pmatrix} \tag{2.66} $$
共分散行列$\Sigma$も同様に $$ \boldsymbol{\Sigma} = \begin{pmatrix} \boldsymbol{\Sigma_{aa}} &amp;amp;&amp;amp; \boldsymbol{\Sigma_{ab}} \\\ \boldsymbol{\Sigma_{ba}} &amp;amp;&amp;amp; \boldsymbol{\Sigma_{bb}} \end{pmatrix} \tag{2.</description>
    </item>
    
    <item>
      <title>機械学習 最小二乗法</title>
      <link>https://gurimusan.github.io/blog/2018/01/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%97%E6%B3%95/</link>
      <pubDate>Tue, 02 Jan 2018 08:49:32 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2018/01/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%97%E6%B3%95/</guid>
      <description>最小二乗法(Least Squares) m個のデータ$(x_{1}, y_{1}),&amp;hellip;,(x_{m}, y_{m})$を直線に当てはめたい。直線の方程式を下記のように置く。
$$ y = \theta_{0}x + \theta_{1}x $$
もしm個のデータ$(x_{1}, y_{1}),&amp;hellip;,(x_{m}, y_{m})$が直線$y^{(i)} = \theta_{0}x^{(i)} + \theta_{1}x^{(i)},\ i=1,&amp;hellip;,m$となっていればデータ$(x^{(i)}, y^{(i)})$は全て直線$y = \theta_{0}x + \theta_{1}x$上にあるが、データは直線には一致しない。そこで
$$ y^{(i)} \approx \theta_{0}x^{(i)} + \theta_{1}x^{(i)}\\
i=1,&amp;hellip;,m $$
となるように近似して、直線のパラメータを定める。次の誤差の二乗和の最小化問題を解くことでパラメータ$\theta$を求める。
$$ min\ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}((\theta_{0}x^{(i)} + \theta_{1}x^{(i)}) - y^{(i)})^{2} $$
これを最小二乗法と呼ぶ。
例 5点$(4, -17), (15, -4), (30, -7), (100, 50), (200, 70)$に最小二乗法で直線を当てはめる。
当てはめる直線を$y = \theta_{0} + \theta_{1}x$とすると、
$$ m = 5\\
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}((\theta_{0}x^{(i)} + \theta_{1}x^{(i)}) - y^{(i)})^{2} $$</description>
    </item>
    
    <item>
      <title>coursera 機械学習 11週目</title>
      <link>https://gurimusan.github.io/blog/2017/12/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-11%E9%80%B1%E7%9B%AE/</link>
      <pubDate>Tue, 26 Dec 2017 13:22:08 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2017/12/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-11%E9%80%B1%E7%9B%AE/</guid>
      <description>アプリケーションの例: 写真の光学文字認識(Application Example: Photo OCR) 写真の光学文字認識(Photo OCR) 問題の説明とパイプライン(Problem Description and Pipeline) 写真の中の文字をコンピューターで読み取る試みを写真OCR、写真の光学文字認識という。
コンピューターで文字を読み取る場合、幾つかの工程(Pipeline)に分けて処理する必要がある。
下記のその手順を記す。
 テキストを探す(Text Detection) 文字の分離(Character Segmentation) 文字の認識(Character Recognition)  これらの工程で、それぞれ目的関数を用意して、教師あり学習(Supervised Learning)を使って学習させる。
テキストを探す(Text Detection) 文字を含んだ画像($y=1$)と文字を含んでいない画像($y=0$)を大量に用意して、アルゴリズムに学習させる。
文字の分離(Character Segmentation) 文字と文字の境界が見える画像($y=1$)と文字と文字の境界を含まない画像($y=0$)を大量に用意して、アルゴリズムに学習させる。
文字の認識(Character Recognition) ラベルと同一の文字を含む画像($y=1$)とラベルと異なる文字を含む画像($y=0$)を用意して、アルゴリズムに学習させる。
スライディングウインドウ(Sliding Windows) 特定のサイズの短形(例えば$10 \times 10$)を画像を端から端まで少しづつずらして走査するスライディングウィンドウ(Sliding Windows)というアルゴリズムを使って、短形の中に文字や人が含まれていないか調べる。
1つの短形(Sliding Windows)だけでは、大き過ぎたり、小さ過ぎたりする可能性があるため、短形(Sliding Windows)は複数用意して走査を行う。
人口的に作った大量データをどのようにして得るか(Getting Lots of Data and Artificial Data) 機械学習において信頼出来る方法の1つに低バイアスな学習アルゴリズムに大量のトレーニングセットを用意して訓練する、という手法がある。
大量にデータを用意することで、良い学習アルゴリズムを得ることができる。
大量にデータを用意する方法としては、文字の認識(Character Recognition)であれば下記の手法がある。
 フォントを変える 文字の大きさを変える 文字を回転させる 文字の位置をずらす 文字を歪める 背景を変える ランダムに選んだピクセルの値を０に置換する  シーリング分析: (Ceiling Analysis: What Part of the Pipeline to Work on Next) 下記の表は各要素と、その要素精度が100%だった時のシステム全体の精度をまとめたものである。</description>
    </item>
    
    <item>
      <title>coursera 機械学習 10週目</title>
      <link>https://gurimusan.github.io/blog/2017/12/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-10%E9%80%B1%E7%9B%AE/</link>
      <pubDate>Tue, 19 Dec 2017 13:19:51 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2017/12/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-10%E9%80%B1%E7%9B%AE/</guid>
      <description>大規模な機械学習(Large Scale Machine Learning) 大きなデータセットでの最急降下法(Gradient Descent with Large Datasets) 大きなデータセットでの学習(Learning With Large Datasets) 線形回帰の最急降下法の更新ルールは下記のとおり。
$$ h_{\theta}(x) = \sum_{j=0}^{n}\theta_{j}x_{x} \\
J_{train} = \frac{1}{2m} \sum_{i=0}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^{2} \\
Repeat \lbrace \\
\hspace{10pt} \theta_{j} := \theta_{j} - \alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)} - y^{(i)}))x_{j}^{(i)} \\
\hspace{10pt} (for\ every\ j=0,&amp;hellip;,n) \\
\rbrace $$
$m=100,000,000$となるトレーニングセットを扱う時、勾配が十分小さな値がとるまで間、最急降下法の1ステップにおいて、1億の和を計算する必要がある。
大規模データを扱う機械学習において、 計算量的にリーズナブルな、または効率的な方法を知りたい。
そこで主に2つのアイディアを紹介する。
1番目は確率的最急降下法(Stochastic Gradient Descent)、2番目はMap Reduceと呼ばれる物だ。
確率的最急降下法(Stochastic Gradient Descent) 確率的最急降下法(Stochastic Gradient Descent)は最急降下法の1ステップにおいて、1つのデータのみを扱い、パラメータを更新する手法である。
下記は確率的最急降下法(Stochastic Gradient Descent)のパラメータ更新のルールである。
$$ \theta_{j} := \theta_{j} - \alpha (h_{\theta}(x^{(i)} - y^{(i)}))x_{j}^{(i)} $$</description>
    </item>
    
    <item>
      <title>coursera 機械学習 8週目</title>
      <link>https://gurimusan.github.io/blog/2017/12/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-8%E9%80%B1%E7%9B%AE/</link>
      <pubDate>Mon, 04 Dec 2017 12:51:59 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2017/12/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-8%E9%80%B1%E7%9B%AE/</guid>
      <description>教師なし学習(Unsupervised Learning) クラスタリング(Clustering) 教師なし学習: イントロダクション(Unsupervised Learning: Introduction) ラベルなしのデータセットから学習することを、教師なし学習(Unsupervised Learning)という。
教師なし学習のトレーニングセットには、ラベルがない。なので教師なし学習では、データセットからデータ構造を探す。
データセットからデータ構造を探すためにクラスタリングというアルゴリズムがある。
データセットからデータ構造を探すためのアルゴリズムは、他にも幾つかあるが、まずはクラスタリングを学ぶ。
K-Meansアルゴリズム(K-Means Algorithm) クラスタリングアルゴリズムであるK-Meansアルゴリズムを学ぶ。
K-Meansアルゴリズムを文章で書くと下記のとおり。
 各クラスタの中心点(Centroid)を決める: データ$x$と同次元にあるK個のクラスタの中心点 $u_1, u_2,&amp;hellip;, u_{K} \in \mathbb{R}^{n}$ を求める データセットの要素をクラスタの割り付ける: 中心点$u_1, u_2,&amp;hellip;, u_{K}$と$x$との距離を求めて、最も距離が近いクラスタを割り付ける クラスタの中心点を更新する: クラスタへ割り付けられた要素$x$の平均を算出して、新たなクラスタの中心点(Centroid)を更新する クラスタの割り付けによる移動がなくなるまで2-3を繰り返す  目的関数の最適化(Optimization Objective) K-Meansアルゴリズムの目的関数は、$x$とクラスタの中心点$u$を使って下記のように定義される。
$$ \displaystyle J(c^{(1)},&amp;hellip;,c^{(m)},u_{1},&amp;hellip;,u_{K}) = \frac{1}{m} \sum_{i=1}^{m} |x^{(i)} - u_{c^{(i)}}| $$
K-Meansアルゴリズムでは、この目的関数を最小化するクラスタを選択することが推奨される。
$$ \displaystyle min_{c^{(1)},&amp;hellip;,c^{(m)},u_{1},&amp;hellip;,u_{K}} J(c^{(1)},&amp;hellip;,c^{(m)},u_{1},&amp;hellip;,u_{K}b $$
ランダム初期化(Random Initialization) 最初にランダムにクラスタをトレーニングセットにピックする。
K-Meansアルゴリズムは局所最適解に収束しうるが、最適解を得るために、K-Meansアルゴリズム100回実行して、その中でコスト関数が最も小さくなるものを選べば良い。
クラスタ数の選択(Choosing the Number of Clusters) クラスタの数はどうやって選ぶのか？
多くの場合は、人が決める。
自動的に決める方法とし、エルボー法というアルゴリズムがある。
エルボー法はクラスタ数$K$を徐々に増やしていき、コスト関数の変化量が一番大きいクラスタ数$K$を選ぶ方法。
ただし、クラスタ数$K$を増やしても、コスト関数の変化量があまりかわらない場合は、クラスタ数を決めることができない。
その場合は、そもそもどういった目的でクラスタを分けたいのか、ということからクラスタ数を決める。
次元数の削減(Dimensionality Reduction) 動機(Motivation) 動機Ⅰ: データ圧縮(Motivation I: Data Compression) データ圧縮は、より少ないメモリやディスク容量にデータを圧縮するだけでなく、学習アルゴリズムのスピードアップにもなる。</description>
    </item>
    
    <item>
      <title>coursera 機械学習 7週目</title>
      <link>https://gurimusan.github.io/blog/2017/12/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-7%E9%80%B1%E7%9B%AE/</link>
      <pubDate>Fri, 01 Dec 2017 14:07:58 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2017/12/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-7%E9%80%B1%E7%9B%AE/</guid>
      <description>Large Margin Classification Optimization objective ロジスティック回帰のコスト関数は下記であった。
$$ min_\theta - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2 $$
この目的関数を最小化するためには、$z=\theta^{T}x$として、下記の条件を満たす$\theta$を学習する必要があった。
 $y=1$のとき、$z \gg 0$ $y=0$のとき、$z \ll 0$  サポートベクターマシン(support vector machine, SVM)のコスト関数では、
 $y=1$の時、$z \gt 1$は$0$ $y=0$の時、$z \lt 1$は$0$  という前提条件をつける。
ロジスティック回帰のコスト関数との比較をグラフにすると下記のとおり。
$y=1$の時 $y=0$の時 コスト関数は下記のようになる。
$$ min_\theta\ \frac{1}{m} \sum^m_{i=1} [y^{(i)} cost_1 (\theta^{\mathrm{T}} x^{(i)}) + (1 - y^{(i)}) cost_0 (\theta^{\mathrm{T}} x^{(i)})] + \frac{\lambda}{2m} \sum^n_{i=1} \theta^2_j $$</description>
    </item>
    
    <item>
      <title>coursera 機械学習 6週目</title>
      <link>https://gurimusan.github.io/blog/2017/11/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-6%E9%80%B1%E7%9B%AE/</link>
      <pubDate>Sun, 26 Nov 2017 13:59:39 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2017/11/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-6%E9%80%B1%E7%9B%AE/</guid>
      <description>学習アルゴリズムの評価(Evaluation a Learning Algorithms) 次に試すものを決める(Deeciding What to Try Next) 誤差が大きい場合、検討すべき点は下記のとおり。
 もっと多くのトレーニングデータを得るか 説明変数を少なくするか 説明変数を多くするか 多次元の説明変数を加えるか $\lambda$を大きくするか $\lambda$を小さくするか  これらは勘で選ばれることが多い。
仮説の評価(Evaluating a Hypothesis) 仮説は誤差が少なければ良いというわけではなく、オーバーフィッティングしている可能性がある。
オーバーフィッティングしていないか検証する手段としては、2次までの線形回帰であればグラフにすれば分かるが、説明変数が多いとグラフ化できない。
データをトレーニングセットとテストセットの2つに分けて、トレーニングセットとテストセットの
 コスト関数 分類問題のミス率  を比較してオーバーフィッティングしていないか判断する。
モデルの選択とトレーニングセット/検証セット/テストセット データを学習用とテスト用とで分けるか？
これらを使って、どうやってモデルを選択するのか？
仮説が新しい学習用データを正確に予想するか、ということは意味しない。
データセットを下記のように分ける。
 トレーニングセット：60％ クロス検証セット：20％ テストセット：20％  これらのデータセットを使うことで、3つの別々のエラー値を計算できるようになる。
 多項式次数毎にトレーニングセットを仕様して、$\Theta$のパラメータを最適する クロス検証セットを仕様して、最小誤差を有する多項式の次数$d$を決める テストセットを用いて、一般化誤差$J_{test}^{\Theta^{(d)}}$を推定する  偏り vs 分散(Bias vs. Variance) 偏りか分散かを判断する(Diagnosing Bias vs. Variance) 目的関数がアンダーフィッティングしているのか、オーバーフィッティングしているのか調べる必要がある。
高バイアス(hight bias)はアンダーフィッティングしている状態で、高分散(hight variance)はオーバーフィッティングしている状態で、これらの黄金比を見つける必要がある。
トレニング誤差$J_{train}(\Theta)$は、次数を増やせば増やすほど減少する傾向にある。
クロス検証誤差$J_{CV}(\Theta)$は、ある地点までは次数を増やせば増やすほど減少していく傾向にあり、ある地点を超えると次数を増やせば増やすほど増加していく傾向にある、すなわち凸曲線になる。
 高バイアス(hight bias): トレニング誤差$J_{train}(\Theta)$とクロス検証誤差$J_{CV}(\Theta)$が高い状態で$J_{CV}(\Theta) \approx J_{train}(\Theta)$ 高分散(hight variance): トレニング誤差$J_{train}(\Theta)$は低く、クロス検証誤差$J_{CV}(\Theta)$が高い状態で$J_{CV}(\Theta) \gt J_{train}(\Theta)$  正規化と偏り／分散(Regularization and Bias/Variance) ある目的関数とコスト関数。</description>
    </item>
    
    <item>
      <title>coursera 機械学習 5週目</title>
      <link>https://gurimusan.github.io/blog/2017/11/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-5%E9%80%B1%E7%9B%AE/</link>
      <pubDate>Sun, 19 Nov 2017 13:20:28 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2017/11/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-5%E9%80%B1%E7%9B%AE/</guid>
      <description>ニューラルネットワーク(Neural Network)のコスト関数 ロジスティック回帰のコスト関数は下記のように定義された。
$$ J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2 \\
$$
ニューラルネットワークのコスト関数は下記のように定義される。
$$ J(\Theta) = - \frac{1}{m} \large[ \sum_{i=1}^m \sum_{k=1}^K y_{k}^{(i)} \log (h_\theta (x^{(i)}))_k + (1 - y^{(i)})\ \log (1 - (h_\theta(x^{(i)}))_k) \large] + \frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_l}\sum_{j=1}^{s_l + 1}(\Theta_{ji}^{(l)})^2 \\
L: レイア数 \\
s_l: l層におけるユニット数 $$
バックプロパゲーション(Backpropagation) 次に、線形回帰やロジスティック回帰と同様に、コスト関数を最小化することを考える。
これは、以下のように各レイアの重み$\Theta$の各要素で$J(\Theta)$を偏微分したもので表される。
$$ \dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta) $$</description>
    </item>
    
    <item>
      <title>coursera 機械学習 4週目</title>
      <link>https://gurimusan.github.io/blog/2017/11/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-4%E9%80%B1%E7%9B%AE/</link>
      <pubDate>Wed, 08 Nov 2017 23:30:22 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2017/11/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-4%E9%80%B1%E7%9B%AE/</guid>
      <description>ニューラルネットワークが必要な背景 ロジスティック回帰によって非線形が可能になった。
ただ、膨大な数の説明変数(feature)が必要なものにロジスティック回帰を適用すると、ロジスティック回帰だと計算が追いつかなくなる。
膨大な数の説明変数(feature)の例としては、写真を見て「これが車であるか否かを判断する」する場合、$50 \times 50$ pixelのグレースケール画像でも、2,500変数で、2次式にしても約3,000,000個のパラメータを計算することになる。(RGBだとその3倍以上)。
そこで、よりよいアルゴリズムを有する人の脳にヒントを得たのがニューラルネットワーク(Neural Network)。
ニューラルネットワーク(Neural Networks) 下記は、入力X($x_1$, $x_2$, $x_3$)に対して、出力Y($h_\Theta(x)$)となる３層構造のニューラルネットワークの例である。
入力でもない、出力でもないレイアを隠れ層(hidden layer)といい、隠れ層(hidden layer)の各ノードをアクティベーションユニット(activation units)という。
$$ a_i^{(j)} = \text{レイア $j$ のアクティベーションユニット $i$} \\
\Theta^{(j)} = \text{レイア $j$ から レイア $j+1$ に移る際に使う重み行列} $$
上記の図のアクティベーションユニット(activation units)、及び出力は下記のように表現される。
$$ a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \\
a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \\
a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \\
h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) $$</description>
    </item>
    
    <item>
      <title>coursera 機械学習 3週目</title>
      <link>https://gurimusan.github.io/blog/2017/11/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-3%E9%80%B1%E7%9B%AE/</link>
      <pubDate>Fri, 03 Nov 2017 05:12:38 +0000</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2017/11/coursera-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-3%E9%80%B1%E7%9B%AE/</guid>
      <description>ロジスティック回帰モデル(Logistic Regression Model) 分類 メール受け取った時、そのメールがスパムか否かを判断するようなことを分類問題という。
スパムの場合は$1$、スパムではない場合は$0$としする。
$0.5$より大きいすべての予測を$1$としてマッピングし、$0.5$未満をすべて0にマッピングする。
$$ h_{ \theta }(x) \geqq 0.5 \to y = 1 $$
$$ h_{ \theta }(x) \lt 0.5 \to y = 0 $$
このように$0$か$1$かのような分類するような問題に、線形回帰を用いて分類を行ったとして、直線への当てはまりが悪くうまくいかない。
そこで、仮説関数$h(x)$が$0 \leqq h(x) \leqq 1$となるような変形を行う。
ロジスティック回帰(Logistic Regression )による仮説関数の表現 線形回帰の仮説関数は、$\theta$の転置行列にベクトル$\boldsymbol{ X }$を掛けたものであった。
$$ h(x) = \theta^{ \mathrm{ T } } \boldsymbol{ X } $$
ロジスティック回帰(Logistic regression)の仮説関数は、$\theta$の転置行列にベクトル$\boldsymbol{ X }$を掛けたものをシグモイド関数(sigmoid function)に代入したものになる。
$$ h_{ \theta }(x) = g(\theta^{ \mathrm{ T } } \boldsymbol{ X }) $$</description>
    </item>
    
    <item>
      <title>Gentoo Linux をインストールする</title>
      <link>https://gurimusan.github.io/blog/2017/03/gentoo-linux-%E3%82%92%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB%E3%81%99%E3%82%8B/</link>
      <pubDate>Wed, 08 Mar 2017 23:52:56 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2017/03/gentoo-linux-%E3%82%92%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB%E3%81%99%E3%82%8B/</guid>
      <description>Gentoo Linux をインストールする 概要 転職し、新しい会社に入社した。
PCを支給してくれるいうことで、macにしようかなと思っていたのだが、折角なのでdockerをまともに使える環境が良いなと思い、linuxベースの開発環境を作ることにした。
そんな折、MacBook Proを捨ててThinkpad T460sを買ってgentooを入れたという記事をはてブで見つけて、gentooベースにすることにした。
PCはノートパソコンで英語配列のものを考え、thinkpadかvaioにしようかと思ったが、thinkpad t460sにした。
t460sには、もともとwindowsがインストールされているが、その上にGentoo Linux をインストールして、デュアルブート環境にする。
Gentoo歴 6〜7年ほど前、家で1年ほど開発機として使っていた。その後、mac book買ったため、現在まで使っていない。
Thinkpad T460S マシンスペック。
 CPU i7 6600U HDD NVMe対応SSD 512GB メモリ 24G 液晶 14型 WQHD 2560×1440 キーボード 英語配列 OS Windows 10 Home  インストールメディアの作成 ダウンロードページからLiveDVDをダンロードして、mac上でUSBにddして焼いた。
$ diskutil unmountDisk /dev/disk2 $ dd if=/Users/gurimusan/Downloads/livedvd-amd64-multilib-20160704.iso of=/dev/disk2 bs=1m $ diskutil eject /dev/disk2  インストール前にやること UEFI のセキュアブートを切る UEFIを操作してセキュアブートを切る。
 Windows と Arch のデュアルブート  高速スタートアップを無効化する &amp;ldquo;コントロールパネル&amp;rdquo;→&amp;rdquo;ハードウェアとサウンド&amp;rdquo;→&amp;rdquo;電源オプション&amp;rdquo;→&amp;rdquo;カバーを閉じたときの動作を選択&amp;rdquo;から無効化する。
Windows上でハードディスクのパーティションを分ける。 gentooをインストールする領域を確保するために、Cドライブが割り当てられているパーティションを縮小して領域を確保する。</description>
    </item>
    
    <item>
      <title>ansibleでEC2インスタンスを作成する</title>
      <link>https://gurimusan.github.io/blog/2016/11/ansible%E3%81%A7ec2%E3%82%A4%E3%83%B3%E3%82%B9%E3%82%BF%E3%83%B3%E3%82%B9%E3%82%92%E4%BD%9C%E6%88%90%E3%81%99%E3%82%8B/</link>
      <pubDate>Tue, 08 Nov 2016 18:35:06 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2016/11/ansible%E3%81%A7ec2%E3%82%A4%E3%83%B3%E3%82%B9%E3%82%BF%E3%83%B3%E3%82%B9%E3%82%92%E4%BD%9C%E6%88%90%E3%81%99%E3%82%8B/</guid>
      <description>やったこと ansibleを使って、aws ec2インスタンスを作成する。
実行環境  Mac OS X El Capitan 10.11.6 python 2.7.12 ansible 2.2.0.0 boto 2.43.0  事前に必要なこと、及び必要なツール  aws上のVPCの構築 IAMユーザ、及びアクセスキー・シークレットキーの入手 pyenv direnv  ディレクトリ構成 . ├── .envrc ├── .python-version ├── hosts │ └── development │ ├── ec2.ini │ ├── ec2.py │ ├── inventry │ └── group_vars │ └── all.yml ├── roles │ └── ec2 │ └── tasks │ ├── instance.yml │ ├── keypair.yml │ ├── main.yml │ └── security_group.</description>
    </item>
    
    <item>
      <title>Raspberry PI で、ADCを使って温度を測定する</title>
      <link>https://gurimusan.github.io/blog/2016/09/raspberry-pi-%E3%81%A7adc%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E6%B8%A9%E5%BA%A6%E3%82%92%E6%B8%AC%E5%AE%9A%E3%81%99%E3%82%8B/</link>
      <pubDate>Thu, 29 Sep 2016 23:52:56 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/2016/09/raspberry-pi-%E3%81%A7adc%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E6%B8%A9%E5%BA%A6%E3%82%92%E6%B8%AC%E5%AE%9A%E3%81%99%E3%82%8B/</guid>
      <description>やりたいこと アナログ温度センサを使って、Raspberry PIで測る。
使用する機器、及び部品 温度センサは MCP9700を利用する。
MCP9700はアナログセンサで、Raspberry PI のピンはデジタルな入力しかできないので、アナログ-デジタル変換を行うために、ADコンバータ MCP3008を利用する。
 Raspberry PI3 Model B 温度センサ MCP9700 ADコンバータ MCP3008 ブレッドボード EIC-801 ジャンパワイア  Raspberry PiでSPIをセットアップする MCP3008は、SPIバスで接続する必要があるため、Raspberry PI のSPIを有効にする。
下記コマンドを実行。
$ sudo raspi-config  Advanced Optionsで A6 SPIを選択して、下記の両方で Yes を選択。
 SPI interface to be enabled? SPI kernel module to be loaded by default?  再起動。
$ sudo reboot  spi_bcm2835 がロードされていることを確認
$ lsmod | grep spi  SPI通信に必要なPythonライブラリをインストール。
$ sudo apt-get install python-pip $ sudo pip install spidev  ブレッドボード配線 温度センサ MCP9700 の配線 データシート1</description>
    </item>
    
    <item>
      <title></title>
      <link>https://gurimusan.github.io/blog/1/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gurimusan.github.io/blog/1/01/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>