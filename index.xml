<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gurimusan blog</title>
    <link>https://gurimusan.github.io/blog/</link>
    <description>Recent content on gurimusan blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-JP</language>
    <lastBuildDate>Fri, 01 Dec 2017 14:07:58 +0900</lastBuildDate>
    
	<atom:link href="https://gurimusan.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>coursera 機械学習 7週目</title>
      <link>https://gurimusan.github.io/blog/post/coursera-machine-learning-week7/</link>
      <pubDate>Fri, 01 Dec 2017 14:07:58 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/post/coursera-machine-learning-week7/</guid>
      <description>Large Margin Classification Optimization objective ロジスティック回帰のコスト関数は下記であった。
$$ min_\theta - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2 $$
この目的関数を最小化するためには、$z=\theta^{T}x$として、下記の条件を満たす$\theta$を学習する必要があった。
 $y=1$のとき、$z \gg 0$ $y=0$のとき、$z \ll 0$  サポートベクターマシン(support vector machine, SVM)のコスト関数では、
 $y=1$の時、$z \gt 1$は$0$ $y=0$の時、$z \lt 1$は$0$  という前提条件をつける。
ロジスティック回帰のコスト関数との比較をグラフにすると下記のとおり。
$y=1$の時 $y=0$の時 コスト関数は下記のようになる。
$$ min_\theta\ \frac{1}{m} \sum^m_{i=1} [y^{(i)} cost_1 (\theta^{\mathrm{T}} x^{(i)}) + (1 - y^{(i)}) cost_0 (\theta^{\mathrm{T}} x^{(i)})] + \frac{\lambda}{2m} \sum^n_{i=1} \theta^2_j $$</description>
    </item>
    
    <item>
      <title>coursera 機械学習 6週目</title>
      <link>https://gurimusan.github.io/blog/post/coursera-machine-learning-week6/</link>
      <pubDate>Sun, 26 Nov 2017 13:59:39 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/post/coursera-machine-learning-week6/</guid>
      <description>学習アルゴリズムの評価(Evaluation a Learning Algorithms) 次に試すものを決める(Deeciding What to Try Next) 誤差が大きい場合、検討すべき点は下記のとおり。
 もっと多くのトレーニングデータを得るか 説明変数を少なくするか 説明変数を多くするか 多次元の説明変数を加えるか $\lambda$を大きくするか $\lambda$を小さくするか  これらは勘で選ばれることが多い。
仮説の評価(Evaluating a Hypothesis) 仮説は誤差が少なければ良いというわけではなく、オーバーフィッティングしている可能性がある。
オーバーフィッティングしていないか検証する手段としては、2次までの線形回帰であればグラフにすれば分かるが、説明変数が多いとグラフ化できない。
データをトレーニングセットとテストセットの2つに分けて、トレーニングセットとテストセットの
 コスト関数 分類問題のミス率  を比較してオーバーフィッティングしていないか判断する。
モデルの選択とトレーニングセット/検証セット/テストセット データを学習用とテスト用とで分けるか？
これらを使って、どうやってモデルを選択するのか？
仮説が新しい学習用データを正確に予想するか、ということは意味しない。
データセットを下記のように分ける。
 トレーニングセット：60％ クロス検証セット：20％ テストセット：20％  これらのデータセットを使うことで、3つの別々のエラー値を計算できるようになる。
 多項式次数毎にトレーニングセットを仕様して、$\Theta$のパラメータを最適する クロス検証セットを仕様して、最小誤差を有する多項式の次数$d$を決める テストセットを用いて、一般化誤差$J_{test}^{\Theta^{(d)}}$を推定する  偏り vs 分散(Bias vs. Variance) 偏りか分散かを判断する(Diagnosing Bias vs. Variance) 目的関数がアンダーフィッティングしているのか、オーバーフィッティングしているのか調べる必要がある。
高バイアス(hight bias)はアンダーフィッティングしている状態で、高分散(hight variance)はオーバーフィッティングしている状態で、これらの黄金比を見つける必要がある。
トレニング誤差$J_{train}(\Theta)$は、次数を増やせば増やすほど減少する傾向にある。
クロス検証誤差$J_{CV}(\Theta)$は、ある地点までは次数を増やせば増やすほど減少していく傾向にあり、ある地点を超えると次数を増やせば増やすほど増加していく傾向にある、すなわち凸曲線になる。
 高バイアス(hight bias): トレニング誤差$J_{train}(\Theta)$とクロス検証誤差$J_{CV}(\Theta)$が高い状態で$J_{CV}(\Theta) \approx J_{train}(\Theta)$ 高分散(hight variance): トレニング誤差$J_{train}(\Theta)$は低く、クロス検証誤差$J_{CV}(\Theta)$が高い状態で$J_{CV}(\Theta) \gt J_{train}(\Theta)$  正規化と偏り／分散(Regularization and Bias/Variance) ある目的関数とコスト関数。</description>
    </item>
    
    <item>
      <title>coursera 機械学習 5週目</title>
      <link>https://gurimusan.github.io/blog/post/coursera-machine-learning-week5/</link>
      <pubDate>Sun, 19 Nov 2017 13:20:28 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/post/coursera-machine-learning-week5/</guid>
      <description>ニューラルネットワーク(Neural Network)のコスト関数 ロジスティック回帰のコスト関数は下記のように定義された。
$$ J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2 \\
$$
ニューラルネットワークのコスト関数は下記のように定義される。
$$ J(\Theta) = - \frac{1}{m} \large[ \sum_{i=1}^m \sum_{k=1}^K y_{k}^{(i)} \log (h_\theta (x^{(i)}))_k + (1 - y^{(i)})\ \log (1 - (h_\theta(x^{(i)}))_k) \large] + \frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_l}\sum_{j=1}^{s_l + 1}(\Theta_{ji}^{(l)})^2 \\
L: レイア数 \\
s_l: l層におけるユニット数 $$
バックプロパゲーション(Backpropagation) 次に、線形回帰やロジスティック回帰と同様に、コスト関数を最小化することを考える。
これは、以下のように各レイアの重み$\Theta$の各要素で$J(\Theta)$を偏微分したもので表される。
$$ \dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta) $$</description>
    </item>
    
    <item>
      <title>coursera 機械学習 4週目</title>
      <link>https://gurimusan.github.io/blog/post/coursera-machine-learning-week4/</link>
      <pubDate>Wed, 08 Nov 2017 23:30:22 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/post/coursera-machine-learning-week4/</guid>
      <description>ニューラルネットワークが必要な背景 ロジスティック回帰によって非線形が可能になった。
ただ、膨大な数の説明変数(feature)が必要なものにロジスティック回帰を適用すると、ロジスティック回帰だと計算が追いつかなくなる。
膨大な数の説明変数(feature)の例としては、写真を見て「これが車であるか否かを判断する」する場合、$50 \times 50$ pixelのグレースケール画像でも、2,500変数で、2次式にしても約3,000,000個のパラメータを計算することになる。(RGBだとその3倍以上)。
そこで、よりよいアルゴリズムを有する人の脳にヒントを得たのがニューラルネットワーク(Neural Network)。
ニューラルネットワーク(Neural Networks) 下記は、入力X($x_1$, $x_2$, $x_3$)に対して、出力Y($h_\Theta(x)$)となる３層構造のニューラルネットワークの例である。
入力でもない、出力でもないレイアを隠れ層(hidden layer)といい、隠れ層(hidden layer)の各ノードをアクティベーションユニット(activation units)という。
$$ a_i^{(j)} = \text{レイア $j$ のアクティベーションユニット $i$} \\
\Theta^{(j)} = \text{レイア $j$ から レイア $j+1$ に移る際に使う重み行列} $$
上記の図のアクティベーションユニット(activation units)、及び出力は下記のように表現される。
$$ a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \\
a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \\
a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \\
h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) $$</description>
    </item>
    
    <item>
      <title>coursera 機械学習 3週目</title>
      <link>https://gurimusan.github.io/blog/post/coursera-machine-learning-week3/</link>
      <pubDate>Fri, 03 Nov 2017 05:12:38 +0000</pubDate>
      
      <guid>https://gurimusan.github.io/blog/post/coursera-machine-learning-week3/</guid>
      <description>ロジスティック回帰モデル(Logistic Regression Model) 分類 メール受け取った時、そのメールがスパムか否かを判断するようなことを分類問題という。
スパムの場合は$1$、スパムではない場合は$0$としする。
$0.5$より大きいすべての予測を$1$としてマッピングし、$0.5$未満をすべて0にマッピングする。
$$ h_{ \theta }(x) \geqq 0.5 \to y = 1 $$
$$ h_{ \theta }(x) \lt 0.5 \to y = 0 $$
このように$0$か$1$かのような分類するような問題に、線形回帰を用いて分類を行ったとして、直線への当てはまりが悪くうまくいかない。
そこで、仮説関数$h(x)$が$0 \leqq h(x) \leqq 1$となるような変形を行う。
ロジスティック回帰(Logistic Regression )による仮説関数の表現 線形回帰の仮説関数は、$\theta$の転置行列にベクトル$\boldsymbol{ X }$を掛けたものであった。
$$ h(x) = \theta^{ \mathrm{ T } } \boldsymbol{ X } $$
ロジスティック回帰(Logistic regression)の仮説関数は、$\theta$の転置行列にベクトル$\boldsymbol{ X }$を掛けたものをシグモイド関数(sigmoid function)に代入したものになる。
$$ h_{ \theta }(x) = g(\theta^{ \mathrm{ T } } \boldsymbol{ X }) $$</description>
    </item>
    
    <item>
      <title>Gentoo Linux をインストールする</title>
      <link>https://gurimusan.github.io/blog/post/gentoo-install-battle/</link>
      <pubDate>Wed, 08 Mar 2017 23:52:56 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/post/gentoo-install-battle/</guid>
      <description>Gentoo Linux をインストールする 概要 転職し、新しい会社に入社した。
PCを支給してくれるいうことで、macにしようかなと思っていたのだが、折角なのでdockerをまともに使える環境が良いなと思い、linuxベースの開発環境を作ることにした。
そんな折、MacBook Proを捨ててThinkpad T460sを買ってgentooを入れたという記事をはてブで見つけて、gentooベースにすることにした。
PCはノートパソコンで英語配列のものを考え、thinkpadかvaioにしようかと思ったが、thinkpad t460sにした。
t460sには、もともとwindowsがインストールされているが、その上にGentoo Linux をインストールして、デュアルブート環境にする。
Gentoo歴 6〜7年ほど前、家で1年ほど開発機として使っていた。その後、mac book買ったため、現在まで使っていない。
Thinkpad T460S マシンスペック。
 CPU i7 6600U HDD NVMe対応SSD 512GB メモリ 24G 液晶 14型 WQHD 2560×1440 キーボード 英語配列 OS Windows 10 Home  インストールメディアの作成 ダウンロードページからLiveDVDをダンロードして、mac上でUSBにddして焼いた。
$ diskutil unmountDisk /dev/disk2 $ dd if=/Users/gurimusan/Downloads/livedvd-amd64-multilib-20160704.iso of=/dev/disk2 bs=1m $ diskutil eject /dev/disk2  インストール前にやること UEFI のセキュアブートを切る UEFIを操作してセキュアブートを切る。
 Windows と Arch のデュアルブート  高速スタートアップを無効化する &amp;ldquo;コントロールパネル&amp;rdquo;→&amp;rdquo;ハードウェアとサウンド&amp;rdquo;→&amp;rdquo;電源オプション&amp;rdquo;→&amp;rdquo;カバーを閉じたときの動作を選択&amp;rdquo;から無効化する。
Windows上でハードディスクのパーティションを分ける。 gentooをインストールする領域を確保するために、Cドライブが割り当てられているパーティションを縮小して領域を確保する。</description>
    </item>
    
    <item>
      <title>ansibleでEC2インスタンスを作成する</title>
      <link>https://gurimusan.github.io/blog/post/how-to-create-ec2-instance-use-ansible/</link>
      <pubDate>Tue, 08 Nov 2016 18:35:06 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/post/how-to-create-ec2-instance-use-ansible/</guid>
      <description>やったこと ansibleを使って、aws ec2インスタンスを作成する。
実行環境  Mac OS X El Capitan 10.11.6 python 2.7.12 ansible 2.2.0.0 boto 2.43.0  事前に必要なこと、及び必要なツール  aws上のVPCの構築 IAMユーザ、及びアクセスキー・シークレットキーの入手 pyenv direnv  ディレクトリ構成 . ├── .envrc ├── .python-version ├── hosts │ └── development │ ├── ec2.ini │ ├── ec2.py │ ├── inventry │ └── group_vars │ └── all.yml ├── roles │ └── ec2 │ └── tasks │ ├── instance.yml │ ├── keypair.yml │ ├── main.yml │ └── security_group.</description>
    </item>
    
    <item>
      <title>Raspberry PI で、ADCを使って温度を測定する</title>
      <link>https://gurimusan.github.io/blog/post/raspberry-pi-how-to-measuring-temperature-using-adc/</link>
      <pubDate>Thu, 29 Sep 2016 23:52:56 +0900</pubDate>
      
      <guid>https://gurimusan.github.io/blog/post/raspberry-pi-how-to-measuring-temperature-using-adc/</guid>
      <description>やりたいこと アナログ温度センサを使って、Raspberry PIで測る。
使用する機器、及び部品 温度センサは MCP9700を利用する。
MCP9700はアナログセンサで、Raspberry PI のピンはデジタルな入力しかできないので、アナログ-デジタル変換を行うために、ADコンバータ MCP3008を利用する。
 Raspberry PI3 Model B 温度センサ MCP9700 ADコンバータ MCP3008 ブレッドボード EIC-801 ジャンパワイア  Raspberry PiでSPIをセットアップする MCP3008は、SPIバスで接続する必要があるため、Raspberry PI のSPIを有効にする。
下記コマンドを実行。
$ sudo raspi-config  Advanced Optionsで A6 SPIを選択して、下記の両方で Yes を選択。
 SPI interface to be enabled? SPI kernel module to be loaded by default?  再起動。
$ sudo reboot  spi_bcm2835 がロードされていることを確認
$ lsmod | grep spi  SPI通信に必要なPythonライブラリをインストール。
$ sudo apt-get install python-pip $ sudo pip install spidev  ブレッドボード配線 温度センサ MCP9700 の配線 データシート1</description>
    </item>
    
  </channel>
</rss>